{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the relevent files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import GRU, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from pickle import dump\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "#from tensorflow import ConfigProto\n",
    "#from tensorflow import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating functions for loading files and creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "\t# load the model\n",
    "\tmodel = InceptionV3()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t# extract features from each photo\n",
    "\tfeatures = dict()\n",
    "\tfor name in listdir(directory):\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = directory + '\\\\' + name\n",
    "\t\timage = load_img(filename, target_size=(299, 299))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the Inception model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# get image id\n",
    "\t\timage_id = name.split('.')[0]\n",
    "\t\t# store feature\n",
    "\t\tfeatures[image_id] = feature\n",
    "\t\tprint('>%s' % name)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"directory = 'C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flicker8k_Dataset'\\nfeature = extract_features(directory)\\nprint('Extracted Features: %d' % len(feature))\\n# save to file\\ndump(feature, open('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\features.pkl', 'wb'))\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"directory = 'C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flicker8k_Dataset'\n",
    "feature = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(feature))\n",
    "# save to file\n",
    "dump(feature, open('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\features.pkl', 'wb'))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading ,cleaning the description and creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## loading file code ####################################################\n",
    "def load_file(doc):\n",
    "    file = open(doc,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "######################## loading description part ##############################################\n",
    "def load_describe(doc):\n",
    "    mapping = dict()\n",
    "    for l in doc.split('\\n'):\n",
    "        tk = l.split(' ')\n",
    "        if(len(l)<2):\n",
    "            continue\n",
    "        img_id,img_des = tk[0],tk[1:]\n",
    "        img_id = img_id.split('.')[0]\n",
    "        img_des = ' '.join(img_des)\n",
    "        \n",
    "        if(img_id not in mapping):\n",
    "            mapping[img_id]=list()\n",
    "        mapping[img_id].append(img_des)\n",
    "    return mapping\n",
    "\n",
    "########################## cleaning the description part ############################################\n",
    "def cleaning_descibe_data(describe):\n",
    "    trans = str.maketrans('','',string.punctuation)\n",
    "    for key,data in describe.items():\n",
    "        for k in range(len(data)):\n",
    "            store = data[k]\n",
    "           \n",
    "            store = store.split()                         # tokenize\n",
    "            \n",
    "            store = [word.lower() for word in store]      # lowering\n",
    "            \n",
    "            store = [word.translate(trans) for word in store] # remove punchuation\n",
    "            \n",
    "            store = [word for word in store if len(word)>1]\n",
    "            \n",
    "            store = [word for word in store if word.isalpha()]\n",
    "            \n",
    "            data[k] = ' '.join(store)\n",
    "            \n",
    "######################## creating vocab ######################\n",
    "\n",
    "def vocab_part(describe):\n",
    "    desc=set()\n",
    "    for key in describe.keys():\n",
    "        [desc.update(des.split()) for des in describe[key]]\n",
    "    return desc\n",
    "\n",
    "##################### saving the description #############\n",
    "def store_describe(describe,f_name):\n",
    "    line=list()\n",
    "    for key,data in describe.items():\n",
    "        for d in data:\n",
    "            line.append(key+ ' ' +d)\n",
    "    data='\\n'.join(line)\n",
    "    file=open(f_name,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = load_file('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flickr8k.token.txt') \n",
    "descript = load_describe(describe)\n",
    "cleaning_descibe_data(descript)\n",
    "vocab = vocab_part(descript)\n",
    "#store_describe(descript,'C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\describe.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the vocabulary length from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len describe 8092\n",
      "len vocab 8680\n"
     ]
    }
   ],
   "source": [
    "print('len describe %d' % len(descript))\n",
    "print('len vocab %d' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descript['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating funtions for loading the description as per the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "\n",
    "\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = pickle.load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "train_des: 6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "file = 'C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flickr_8k.trainImages.txt'\n",
    "train = load_set(file)\n",
    "print('Dataset: %d' % len(train))\n",
    "train_des = load_clean_descriptions('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\describe.txt',train)\n",
    "print('train_des: %d' % len(train_des))\n",
    "\n",
    "train_feature = load_photo_features('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\features.pkl',train)\n",
    "print('Photos: train=%d' % len(train_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    " \n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tokenize = create_tokenizer(train_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(t_tokenize,open('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\t_tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 7507\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_des)\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('vocab_size: %d'%vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating sequences for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the max length for which we can generate the caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(train_des)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now defining the model for image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(2048,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = GRU(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\t#keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train,x2_train,ytrain = create_sequences(tokenizer , max_length, train_des, train_feature, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "test_des: 1000\n",
      "Photos: train=1000\n"
     ]
    }
   ],
   "source": [
    "file = 'C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flickr_8k.testImages.txt'\n",
    "test = load_set(file)\n",
    "print('Dataset: %d' % len(test))\n",
    "test_des = load_clean_descriptions('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\describe.txt',test)\n",
    "print('test_des: %d' % len(test_des))\n",
    "\n",
    "test_feature = load_photo_features('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\features.pkl',test)\n",
    "print('Photos: train=%d' % len(test_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_test,x2_test,ytest = create_sequences(tokenizer,max_length,test_des,test_feature,vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final structure of model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 256)      1921792     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 33, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 256)          393984      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7507)         1929299     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,835,411\n",
      "Trainable params: 4,835,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 2048)\n",
      "(47, 33)\n",
      "(47, 7507)\n"
     ]
    }
   ],
   "source": [
    "# test the data generator\n",
    "generator = data_generator(train_des, train_feature, tokenizer, max_length, vocab_size)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2322s 387ms/step - loss: 4.5220\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1722s 287ms/step - loss: 3.7394\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2141s 357ms/step - loss: 3.4785\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2591s 432ms/step - loss: 3.3290\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2583s 430ms/step - loss: 3.2310\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1714s 286ms/step - loss: 3.1582\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1723s 287ms/step - loss: 3.1087\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2778s 463ms/step - loss: 3.0715\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2271s 378ms/step - loss: 3.0426\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2143s 357ms/step - loss: 3.0198\n",
      "Epoch 1/1\n",
      " 444/6000 [=>............................] - ETA: 26:27 - loss: 2.9996"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "steps = len(train_des)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_des, train_feature, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\model_weights\\\\model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\model_weights\\\\model_9.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_id(num,tokenizer):\n",
    "    for word,idx in tokenizer.word_index.items():\n",
    "        if(idx == num):\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_describe(model,tokenizer,photo,max_length):\n",
    "    txt = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        seq = tokenizer.texts_to_sequences([txt])[0]\n",
    "        seq = pad_sequences([seq],maxlen=max_length)\n",
    "        pred= model.predict([photo,seq],verbose=0)\n",
    "        pred=np.argmax(pred)\n",
    "        \n",
    "        word = word_id(pred,tokenizer)\n",
    "        \n",
    "        if(word is None):\n",
    "            break\n",
    "        txt += ' '+ word\n",
    "        if(word == 'endseq'):\n",
    "            break\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "\tactual, predicted = list(), list()\n",
    "\t# step over the whole set\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# generate description\n",
    "\t\tyhat = generate_describe(model, tokenizer, photos[key], max_length)\n",
    "\t\t# store actual and predicted\n",
    "\t\treferences = [d.split() for d in desc_list]\n",
    "\t\tactual.append(references)\n",
    "\t\tpredicted.append(yhat.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaurav\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "f_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.513198\n",
      "BLEU-2: 0.279901\n",
      "BLEU-3: 0.187234\n",
      "BLEU-4: 0.080185\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(f_model,test_des,test_feature,tokenizer,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_t = pickle.load(open('C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\t_tokenizer.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filename):\n",
    "\t# load the model\n",
    "\tmodel = InceptionV3()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# load the photo\n",
    "\timage = load_img(filename, target_size=(299, 299))\n",
    "\t# convert the image pixels to a numpy array\n",
    "\timage = img_to_array(image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the Inception model\n",
    "\timage = preprocess_input(image)\n",
    "\t# get features\n",
    "\tfeature = model.predict(image, verbose=0)\n",
    "\treturn feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='C:\\\\Users\\\\gaurav\\\\miniconda3\\\\envs\\\\Datasets\\\\Flickr8k\\\\Flicker8k_Dataset\\\\232874193_c691df882d.jpg'\n",
    "photo = extract_features(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(file)\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = generate_describe(f_model,tokenizer_t,photo,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq dog is running through the grass endseq\n"
     ]
    }
   ],
   "source": [
    "print(describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### done with this project #################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
